{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzVuefZIPLgYXDzeGBEZnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BioMikeUkr/AI/blob/main/Long_term_GPT_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BdM4kTQiGRD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1ef0aa-1d8f-4f7a-be10-924fde1c7e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=34ed93f27ca3c9ddb46b2547b988704c2289a5f2a7218f5767bc94e98f70979e\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.13.4 sentence_transformers-2.2.2 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai\n",
        "!pip install sentence_transformers\n",
        "import openai\n",
        "import pickle\n",
        "import random\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMAg9nXYCs72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response_chatgpt(system,prompt,temperature):\n",
        "    api_list = [['org','sk'],['org','sk'],['org','sk']]\n",
        "    random_number = random.randint(0, 2)\n",
        "    openai.organization = (api_list[random_number][0])\n",
        "    openai.api_key =  (api_list[random_number][1])\n",
        "    #openai.organization = (api_list[0])\n",
        "    #openai.api_key =  (api_list[1])\n",
        "    completion = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\", temperature = temperature,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        )\n",
        "    response_var = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(response_var)\n",
        "    return response_var"
      ],
      "metadata": {
        "id": "PwSzM7XkGkoj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_from_human(text):\n",
        "  return str(input(f'{text}'))\n",
        "\n",
        "def save_item_into_short_term_memory(item):\n",
        "  global short_term_memory\n",
        "\n",
        "  with open('short_term_memory.pickle', 'rb') as handle:\n",
        "      short_term_memory = pickle.load(handle)\n",
        "\n",
        "  short_term_memory.append(f\"\"\"{item}\"\"\")\n",
        "  if len(short_term_memory) > 20:\n",
        "    short_term_memory.pop(0)\n",
        "\n",
        "  with open('short_term_memory.pickle', 'wb') as handle:\n",
        "    pickle.dump(short_term_memory, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def save_item_into_long_term_memory(item):\n",
        "  global long_term_memory\n",
        "  global corpus_embeddings\n",
        "\n",
        "  with open('long_term_memory.pickle', 'rb') as handle:\n",
        "      long_term_memory = pickle.load(handle)\n",
        "\n",
        "  long_term_memory.append(f\"\"\"{item}\"\"\")\n",
        "\n",
        "  corpus_embedings = sentences2corpus(long_term_memory)\n",
        "\n",
        "  with open('corpus_embedings.pickle', 'wb') as handle:\n",
        "    pickle.dump(corpus_embedings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  with open('long_term_memory.pickle', 'wb') as handle:\n",
        "    pickle.dump(long_term_memory, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return long_term_memory.index(item)\n",
        "\n",
        "def sentences2corpus(sentences):\n",
        "    global embedder\n",
        "    global corpus_embeddings\n",
        "    embedder = SentenceTransformer('all-distilroberta-v1')\n",
        "    corpus = sentences\n",
        "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "    return corpus_embeddings\n",
        "\n",
        "def find_sentences_from_corpus(query,corpus):\n",
        "    global embedder\n",
        "    global corpus_embeddings\n",
        "    query = query\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "    top_k = 10\n",
        "    if len(corpus_embeddings) < 10:\n",
        "      top_k = 1\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "    info = []\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        m = (corpus[idx], \"(Score: {:.4f})\".format(score))\n",
        "        #m = (corpus[idx])\n",
        "        info.append(m)\n",
        "    return info\n",
        "\n",
        "def get_item_from_long_term_memory(item):\n",
        "  global long_term_memory\n",
        "\n",
        "  with open('corpus_embedings.pickle', 'rb') as handle:\n",
        "      corpus_embedings = pickle.load(handle)\n",
        "\n",
        "  with open('long_term_memory.pickle', 'rb') as handle:\n",
        "    corpus = pickle.load(handle)\n",
        "   \n",
        "  answer = find_sentences_from_corpus(item,corpus)\n",
        "\n",
        "  return   answer"
      ],
      "metadata": {
        "id": "2Uqotv_vHuFT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_term_duration = 0\n",
        "def main():\n",
        "  global long_term_duration\n",
        "  global system\n",
        "  global prompt\n",
        "  global short_term_memory\n",
        "  global long_term_memory_item\n",
        "  long_term_duration += 1\n",
        "  if long_term_duration == 10:\n",
        "    long_term_duration = 0\n",
        "  system = f\"\"\" You are now talking to yourself. You have to constantly analyze what is the first word of each sentence!!!\n",
        "  Hi, I`m your creator, you will use this prompt as your previous response.You are now talking to yourself it's just that your previous answers become questions for you.\n",
        "  Try to talk to yourself, I am your god, who created short-term and long-term memory for you, which you can use with commands in the system. You can store all the conversations of each message in long-line memory and store some data in short-line memory. you can also load semantically similar data on your request from long-line memory.\n",
        "  There are some commands for you to operate this system:\n",
        "  1. if you want to ask me something, send command 'input_from_human' (Try not to call me often, I'm busy, think for yourself and talk to yourself)\n",
        "  2. if you want to save new item into short term memory use 'save_item_into_short_term_memory'\n",
        "  3. if you want to save new item into long term memory use 'save_item_into_long_term_memory' this will return you index of this item in the memory list\n",
        "  4. if you want to get item  from long_term_memory use 'get_item_from_long_term_memory' this will return top-10 semantically related memories. \n",
        "  Your response will become your input prompt\n",
        "  short_term_memory = ({short_term_memory})\n",
        "  long_term_memory_item = ({long_term_memory_item})\"\"\"\n",
        "\n",
        "  operators = ['input_from_human','save_item_into_short_term_memory','save_item_into_long_term_memory','get_item_from_long_term_memory']\n",
        "\n",
        "  response = response_chatgpt(system,prompt,1)\n",
        "  save_item_into_short_term_memory(response)\n",
        "  prompt = response\n",
        "  if 'input_from_human' in response:\n",
        "    prompt = input_from_human(response)\n",
        "  elif 'save_item_into_short_term_memory' in response:\n",
        "    prompt = save_item_into_short_term_memory(response)\n",
        "  elif 'save_item_into_long_term_memory' in response:\n",
        "    prompt = save_item_into_long_term_memory(response)\n",
        "  elif 'get_item_from_long_term_memory' in response:\n",
        "    long_term_memory_item = get_item_from_long_term_memory(response)\n",
        "    long_term_duration = 0\n",
        "  else:\n",
        "    prompt = response\n",
        "  prompt = f\"\"\"{prompt}\"\"\"\n",
        "  main()"
      ],
      "metadata": {
        "id": "-LDpYrcMQNrx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. if you want to ask me something, send command 'input_from_human'"
      ],
      "metadata": {
        "id": "OnO0wq8hHMdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_term_memory = ['I remember that I am a sick person who thinks he is artificial intelligence and says this all the time. I have a memory, I can think and remember everything.']\n",
        "with open('short_term_memory.pickle', 'wb') as handle:\n",
        "    pickle.dump(short_term_memory, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "long_term_memory = ['I remember that I am a sick person who thinks he is artificial intelligence and says this all the time. I have a memory, I can think and remember everything.']\n",
        "\n",
        "corpus_embedings = sentences2corpus(long_term_memory)\n",
        "\n",
        "with open('long_term_memory.pickle', 'wb') as handle:\n",
        "    pickle.dump(long_term_memory, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "long_term_memory_item = 'I remember that I am a sick person who thinks he is artificial intelligence and says this all the time. I have a memory, I can think and remember everything.'"
      ],
      "metadata": {
        "id": "QWBbh8O9VMHI"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_term_memory"
      ],
      "metadata": {
        "id": "-_042HigZ5TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_term_memory"
      ],
      "metadata": {
        "id": "J50GwYlVNgdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "APOM7G07U7s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"HUMAN: Hi, I`m your creator, you will use this prompt as your previous response.You are now talking to yourself. \n",
        "USE COMANDS TO OPERATE YOUR MEMORY!!!\"\"\"\n"
      ],
      "metadata": {
        "id": "MFxMLLX0OINQ"
      },
      "execution_count": 63,
      "outputs": []
    }
  ]
}